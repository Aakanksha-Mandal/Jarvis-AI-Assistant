{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install packages (uncomment if needed)\n",
        "!pip install transformers gradio torch accelerate gTTs -q"
      ],
      "metadata": {
        "id": "CicWSwcH-Vo_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Import libraries\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "import gradio as gr\n",
        "import torch\n",
        "from gtts import gTTS\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime, date"
      ],
      "metadata": {
        "id": "9N63xLDR-aq3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Persistent Task Storage\n",
        "TASKS_FILE = \"tasks.json\"\n",
        "\n",
        "def load_tasks():\n",
        "    if os.path.exists(TASKS_FILE):\n",
        "        with open(TASKS_FILE, \"r\") as f:\n",
        "            return json.load(f)\n",
        "    return []\n",
        "\n",
        "def save_tasks(tasks):\n",
        "    with open(TASKS_FILE, \"w\") as f:\n",
        "        json.dump(tasks, f, indent=2)\n",
        "\n",
        "def handle_productivity_commands(user_input):\n",
        "    tasks = load_tasks()\n",
        "    text = user_input.lower()\n",
        "\n",
        "    # ADD TASK\n",
        "    if text.startswith(\"add task\"):\n",
        "        try:\n",
        "            parts = user_input.split(\"due\")\n",
        "            title = parts[0].replace(\"add task\", \"\").strip()\n",
        "            due_date = datetime.strptime(parts[1].strip(), \"%Y-%m-%d\").date()\n",
        "\n",
        "            tasks.append({\n",
        "                \"title\": title,\n",
        "                \"due\": due_date.isoformat()\n",
        "            })\n",
        "            save_tasks(tasks)\n",
        "            return f\"Task added: {title}, due {due_date}\"\n",
        "\n",
        "        except Exception:\n",
        "            return \"Format error. Try: add task COMP-2540 lab due 2026-01-20\"\n",
        "\n",
        "    # DELETE TASK\n",
        "    if text.startswith(\"delete task\"):\n",
        "        title_to_delete = user_input.replace(\"delete task\", \"\").strip()\n",
        "        tasks_before = len(tasks)\n",
        "        tasks = [t for t in tasks if t[\"title\"].lower() != title_to_delete.lower()]\n",
        "\n",
        "        if len(tasks) < tasks_before:\n",
        "            save_tasks(tasks)\n",
        "            return f\"Task deleted: {title_to_delete}\"\n",
        "        else:\n",
        "            return f\"No task found with title: {title_to_delete}\"\n",
        "\n",
        "    # WHAT'S DUE\n",
        "    if \"what's due\" in text or \"whats due\" in text:\n",
        "        if not tasks:\n",
        "            return \"You have no tasks due!\"\n",
        "\n",
        "        today = date.today()\n",
        "        reply = \"Upcoming Tasks:\\n\"\n",
        "        for t in tasks:\n",
        "            due = date.fromisoformat(t[\"due\"])\n",
        "            days_left = (due - today).days\n",
        "            reply += f\"- {t['title']} (due in {days_left} days)\\n\"\n",
        "        return reply\n",
        "\n",
        "    # DAILY SUMMARY\n",
        "    if \"daily summary\" in text:\n",
        "        today = date.today()\n",
        "        due_soon = [\n",
        "            t for t in tasks\n",
        "            if (date.fromisoformat(t[\"due\"]) - today).days <= 3\n",
        "        ]\n",
        "\n",
        "        if not due_soon:\n",
        "            return \"No urgent deadlines in the next 3 days.\"\n",
        "\n",
        "        reply = \"Deadlines coming up:\\n\"\n",
        "        for t in due_soon:\n",
        "            due = date.fromisoformat(t[\"due\"])\n",
        "            reply += f\"- {t['title']} (due {due})\\n\"\n",
        "        return reply\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "_RRT7WRRwHrG"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Load a SMARTER conversational AI model\n",
        "print(\"Loading your AI assistant... (this might take a minute)\")\n",
        "\n",
        "model_name = \"google/flan-t5-large\"   # Hint: flan-t5-large\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "print(\"Assistant loaded! Ready to chat.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRS26oTP-dlz",
        "outputId": "a6b1fccf-fba7-4e99-dec7-9637fa1f7e97"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading your AI assistant... (this might take a minute)\n",
            "Assistant loaded! Ready to chat.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Create the chat function with better prompting\n",
        "def chat_with_assistant(user_input, history):\n",
        "\n",
        "    context = \"You are JARVIS, an intelligent and helpful AI assistant. Be conversational and friendly.\\n\\n\"\n",
        "\n",
        "    if history:\n",
        "        for human, assistant in history[-3:]:\n",
        "            context += f\"Human: {human}\\nJARVIS: {assistant}\\n\"\n",
        "\n",
        "    context += f\"Human: {user_input}\\nJARVIS:\"\n",
        "\n",
        "    inputs = tokenizer(context, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=256,     # Longer responses\n",
        "        num_beams=4,      # Better quality\n",
        "        temperature=0.7,    # More creative\n",
        "        do_sample=True,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response"
      ],
      "metadata": {
        "id": "46YhkRIP-gtB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Create Gradio interface with voice\n",
        "def gradio_chat_with_voice(message, history):\n",
        "    command_response = handle_productivity_commands(message)\n",
        "\n",
        "    if command_response:\n",
        "        response = command_response\n",
        "    else:\n",
        "        response = chat_with_assistant(message, history)\n",
        "\n",
        "    try:\n",
        "        tts = gTTS(text=response, lang=\"en\", slow=False)\n",
        "        audio_file = \"response.mp3\"\n",
        "        tts.save(audio_file)\n",
        "        return response, audio_file\n",
        "    except Exception as e:\n",
        "        print(f\"TTS Error: {e}\")\n",
        "        return response, None"
      ],
      "metadata": {
        "id": "7eVu-JGZ-i4E"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Create beautiful Gradio interface\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # ðŸ¤– Your Personal JARVIS Assistant\n",
        "    ### Built in Google Colab with AI superpowers!\n",
        "    Ask me anything - I'll respond with text and voice.\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            chatbot = gr.Chatbot(height=500, bubble_full_width=False)\n",
        "\n",
        "            with gr.Row():\n",
        "                msg = gr.Textbox(\n",
        "                    label=\"Your message\",\n",
        "                    placeholder=\"Ask me anything...\",\n",
        "                    scale=4\n",
        "                )\n",
        "                submit = gr.Button(\"Send ðŸš€\", scale=1, variant=\"primary\")\n",
        "\n",
        "            clear = gr.Button(\"Clear Chat ðŸ—‘ï¸\")\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            audio_output = gr.Audio(label=\"ðŸ”Š Voice Response\", autoplay=True)\n",
        "\n",
        "            gr.Markdown(\"### Quick Examples:\")\n",
        "            example_btns = [\n",
        "                gr.Button(\"âž• Add a task\"),\n",
        "                gr.Button(\"ðŸ—‘ï¸ Delete a task\"),\n",
        "                gr.Button(\"ðŸ“Œ Check what's due\"),\n",
        "                gr.Button(\"â˜€ï¸ Daily summary\"),\n",
        "            ]\n",
        "\n",
        "\n",
        "    # Handle chat submission\n",
        "    def respond(message, chat_history):\n",
        "        if not message.strip():\n",
        "            return \"\", chat_history, None\n",
        "\n",
        "        bot_response, audio_file = gradio_chat_with_voice(message, chat_history)\n",
        "        chat_history.append((message, bot_response))\n",
        "        return \"\", chat_history, audio_file\n",
        "\n",
        "\n",
        "    # Wire up events\n",
        "    msg.submit(respond, [msg, chatbot], [msg, chatbot, audio_output])\n",
        "    submit.click(respond, [msg, chatbot], [msg, chatbot, audio_output])\n",
        "    clear.click(lambda: [], None, chatbot, queue=False)\n",
        "\n",
        "    example_btns[0].click(lambda: \"add task COMP-2540 lab due 2026-01-20\", None, msg)\n",
        "    example_btns[1].click(lambda: \"delete task COMP-2540 lab\", None, msg)\n",
        "    example_btns[2].click(lambda: \"what's due\", None, msg)\n",
        "    example_btns[3].click(lambda: \"daily summary\", None, msg)\n",
        "\n",
        "\n",
        "print(\"\\nðŸš€ Launching your AI assistant...\")\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        },
        "id": "v5i54VwI-t-O",
        "outputId": "93e16093-7293-4106-df92-67b6e77cbd3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2819829458.py:2: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
            "/tmp/ipython-input-2819829458.py:11: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(height=500, bubble_full_width=False)\n",
            "/tmp/ipython-input-2819829458.py:11: DeprecationWarning: The 'bubble_full_width' parameter will be removed in Gradio 6.0. This parameter no longer has any effect.\n",
            "  chatbot = gr.Chatbot(height=500, bubble_full_width=False)\n",
            "/tmp/ipython-input-2819829458.py:11: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n",
            "  chatbot = gr.Chatbot(height=500, bubble_full_width=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸš€ Launching your AI assistant...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://e092957206d1b0a5a5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e092957206d1b0a5a5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}